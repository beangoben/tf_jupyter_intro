{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "## code from aymericdamien/TensorFlow-Examples\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.38039219  0.37647063\n",
      "   0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.35294119  0.5411765\n",
      "   0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "   0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "   0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "   0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "   0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "   0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "   0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "   0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "   0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.44313729\n",
      "   0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "   0.12156864  0.          0.          0.          0.          0.7843138\n",
      "   0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.18823531\n",
      "   0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "sample_img, _ = mnist.train.next_batch(1)\n",
    "print(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters:\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network parameters:\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "n_input = 784   # MNIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'encoder_h1' : tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1' : tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2' : tf.Variable(tf.random_normal([ n_hidden_1, n_input]))\n",
    "}\n",
    "\n",
    "\n",
    "bias = {\n",
    "    'encoder_b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2' : tf.Variable(tf.random_normal([n_input]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder:\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.matmul(x, weights['encoder_h1'])+bias['encoder_b1'])\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),bias['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.matmul(x, weights['decoder_h1'])+bias['decoder_b1'])\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),bias['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing regular model:\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our predictions:\n",
    "y_pred = decoder_op\n",
    "y_true = X   # true value should be itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss and optimizer:\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# innitalizing the variables:\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch_num = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        for ii in range(total_batch_num):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # run optimization op (backprop) and cost op\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X : batch_xs})\n",
    "        # display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:04d} cost= {:.9f}\".format(epoch+1, c))\n",
    "    \n",
    "    print('Optimization finished')\n",
    "    \n",
    "    # Applying encode and decode over test set:\n",
    "    encode_decode = sess.run( y_pred, feed_dict = {X : mnist.test.images[:examples_to_show]})\n",
    "    \n",
    "    # show original images and reconstruction:\n",
    "    f, a = plt.subplots(2, 10, figsize=(10,2))\n",
    "    for i in range(examples_to_show):\n",
    "        a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "        a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))\n",
    "    f.show()\n",
    "    plt.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

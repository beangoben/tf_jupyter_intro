{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Regression</a></div><div class=\"lev1 toc-item\"><a href=\"#Neural-Network\" data-toc-modified-id=\"Neural-Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Neural Network</a></div><div class=\"lev1 toc-item\"><a href=\"#Convolutional-Networks\" data-toc-modified-id=\"Convolutional-Networks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Convolutional Networks</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data',one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "We'll apply a basic layer of weights to the inputs. Here, the inputs will be all 28 x 28 = 784 pixels of the MNIST picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf notation for permanents. None allows flexibility in that dimension\n",
    "x = tf.placeholder(tf.float32,[None,784]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directly go from inputs to outputs\n",
    "W_linreg = tf.Variable(tf.random_normal([784, 10], stddev = 0.1)) \n",
    "b_linreg = tf.Variable(tf.random_normal([10], stddev = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our prediction\n",
    "y = tf.nn.softmax(tf.matmul(x,W_linreg) + b_linreg)\n",
    "\n",
    "# correct answers\n",
    "y_ = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different optimizers are available, here we use a basic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set as negative log like\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) \n",
    "# 0.5 is the learn rate\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very important! Without this line, none of the cells will be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## begin the training:\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={ x : batch_xs, y_: batch_ys})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### checking the results:\n",
    "# check by comparing max values in vectors\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x : mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 92%. We ought to be better... the best algorithms get ~99% : http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Let's try again, this time with a neural network.\n",
    "To make it a neural network, we'll give it a hidden layer (i.e. add an additional set of weights) and apply non-linear transformations after each multiplication by a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf notation for permanents. None allows flexibility in that dimension\n",
    "x = tf.placeholder(tf.float32,[None,784]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters to consider:\n",
    "batch_size = 200\n",
    "learn_rate = 0.5\n",
    "h1_layer_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go from inputs to hidden layer\n",
    "W_NN1 = tf.Variable(tf.random_normal([784, h1_layer_size], stddev = 0.1)) \n",
    "b_NN1 = tf.Variable(tf.random_normal([h1_layer_size], stddev = 0.1))\n",
    "# Now go from hidden layer to \n",
    "W_NN2 = tf.Variable(tf.random_normal([h1_layer_size, 10], stddev = 0.1)) \n",
    "b_NN2 = tf.Variable(tf.random_normal([10], stddev = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our prediction, apply some relu non-linearity to system.\n",
    "h1_layer = tf.nn.relu(tf.matmul(x,W_NN1) + b_NN1) \n",
    "y = tf.nn.softmax(tf.matmul(h1_layer, W_NN2)+b_NN2)\n",
    "# correct answers\n",
    "y_ = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different optimizers are available, here we use a basic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) # set as negative log like\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) # 0.5 is the learn rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very important! Without this line, none of the cells will be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## begin the training:\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={ x : batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### checking the results:\n",
    "# check by comparing max values in vectors\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x : mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better after just one hidden layer, accuracy up to 97.6% from 92% for linear regression. Can we do still better?\n",
    "\n",
    "Parameters to play around with:\n",
    "- batch size\n",
    "- hidden layer size\n",
    "- Number of hidden layers (try adding another hidden layer on your own!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks allow us to reuse some of the parameters. Instead of requiring weights for each of the input pixels, we have a set of weights that run over all the pixels and get reused. We also try to condense the representation so there are fewer overall 'nodes' in our representation, but each node has its own features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper function to help us set up the convolutional network\n",
    "# strides : Describe how to move the convolutional window on the input\n",
    "# padding : Whether to add extra zero-columns so that the window can be read to the last input column\n",
    "#     'SAME' - indicates extra zero columns will be added\n",
    "#     'VALID - indicates no extra columns will be added, so the number of columns is N_width - W_width \n",
    "#              in the new representation\n",
    "\n",
    "def weight_variable(size):\n",
    "    initial = tf.truncated_normal(size, stddev = 0.02)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def bias_variable(size):\n",
    "    initial = tf.constant(0.1, shape=size)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):  \n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME') \n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the appropriate layers\n",
    "# First, it's necessary to create a 4-tensor out of inputs\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with a convolutional network, our weight/bias convolution only requires (5 x 5 + 1) * 32 = 832 weights.\n",
    "\n",
    "If we had wanted a fully connected neural network going from all input 784 pixels to a hidden layer of size 32, plus a bias, we'd need (784 + 1 ) *32 = 25,120 weights instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # this layer will be a 28 x 28 x 32 rep.\n",
    "h_pool1 = max_pool_2x2(h_conv1)     # After the pooling, we now have a 14 x 14 x 32 rep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a second layer:\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  # this layer will be 28 x 28 x 64\n",
    "h_pool2 = max_pool_2x2(h_conv2)      # Now it will be 7 x 7 x 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's create a fully connected neural network layer\n",
    "W_fc1  = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# we will flatten the convolutional layers we had developed previously\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Last set of weights to get to the output layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# Output:\n",
    "y_conv1 = tf.nn.softmax(tf.matmul(h_fc1, W_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# begin training, this time using fancier Adam optimizer\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv1), reduction_indices=[1])) # set as negative log like\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv1,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some training parameters:\n",
    "num_iters = 2000\n",
    "batch_size = 300\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(num_iters):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0 :\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={ x: batch[0], y_: batch[1]})\n",
    "        print('Step %d, trainig accuracy %g'%(i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_:batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_accuracy = sess.run(accuracy, feed_dict = {x:mnist.test.images, y_:mnist.test.labels})\n",
    "print('Test accuracy for convnet without dropout: %g'%(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, what happened, we only have 10% accuracy on the test set, whereas we had nearly 100% accuracy on the training set. This indicates that the results have been overfit. Let's try it again with dropout.\n",
    "\n",
    "Since dropout randomly deletes nodes from the hidden layer in the fully connected layer, that should help the nodes be a little more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up parameter for dropout:\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Last set of weights to get to the output layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# Output:\n",
    "y_conv2 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# begin training, this time using fancier Adam optimizer\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv2), reduction_indices=[1])) # set as negative log like\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv2,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some training parameters:\n",
    "num_iters = 2000\n",
    "batch_size = 300\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(num_iters):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0 :\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={ x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print('Step %d, training accuracy %g'%(i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_:batch[1], keep_prob: 0.5}) # during trainin, only keep nodes half the tiem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check: are these all running in the same session? or a different one each time you set 'initialize_all_variables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_accuracy = sess.run(accuracy, feed_dict = {x:mnist.test.images, y_:mnist.test.labels, keep_prob: 1.0})\n",
    "print('Test accuracy for convnet without dropout: %g'%(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.4%, That's much better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
